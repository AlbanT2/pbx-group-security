---
draft  : true
title  : Notes about the Malware incident in the factory 
author : Dinis Cruz
notes:
  - english and structure needs fixing
---

Notes about the Malware incident in the factory 

So, it's Sunday the seventeenth of December, and we are four days into the incident and there still a large number of questions that we still don't know about. This incident really highlighted security gaps that we have in group security, and some of the visibility that all operations do. And also presents a great opportunity to do something about it. 

So, basically, what we know, is there is a bunch of DNS calls being made to N points that have being recognized to belong to a commanding control implant. 
One of the question I still have, is how that DNS is being resolved. Because that is still not fully clear, we haven't really connected those dots. And, when we looked at one of the machines, which I think was discovered because Wireshark was installed in one box, is that there was a windows machine that was infected with it. And when we looked at that machine, VNC or RDP, we went to see a whole number of antivirus alerts, which pointed again to the same virus. This anti-virus is actually set to detect not to block, which is kind of a weird thing, that also shows that we not having a central repository to control those alerts, because we should be acting at it much more quickly.

So, the steps that we took at this stage, was that we blocked the machine, we took the machine offline. We rebuild, we took a copy of that machine and we were also able to block the firewall level, the ipâ€™s of those that have being connected. What happen is that when we look at now at the alerts and also the dot trace traffic, we basically, have also seems that are still protocols being made. So, this means that we know at the moment there is still infected machines in the network that we still don't know about. 

And, at this stage, even question is, how many spy machines exit in the domain, where they are, how to the visualize, who they connect to, these are still not easy answers. I think team was able to collect all the spy machines but even now, it was not a simple exercise. There is a whole number of tools that already exist in the market, especially for all the windows at stake, that are very useful, that we really need to get our hands on. But even deploying them, and installing them, seems to take ages, which is not good.

At this stage, we really need to get a grip in understanding what else is infected, what else is on the network. And try to also figure, which one is station zero, because we still don't know if the machine that we found was the original one, and we don't know how other got it then.

Also, looking at past traffic, we also seen that there's being activity since, actually, early November, which is being slipping to the radar, which is also not good. So, we need to try to figure out what happened to that, and understanding the traffic, and understanding actually what is the original source of infection, and what data do we have from that particular period, and from the logs that exist, etc., which is something that is still not fully understood.

So, I'll say that stuff from this week, we need to really get the understanding of what machines are infected, what is the impact, and how to, then, clean up. One thing that we are paying attention is on traffic, so those seem that there is a huge amount of unidentified traffic, but it will be good to double check, if that's exactly the case in the situation where data is being extracted or the command of control is actually being manipulated. The machines at the moment don't seem to go to connect to the internet, but that is simply because the gateway is not set, so one of the things we need to check is whether the gateways is actually being set on purpose, or if there is actually traffic that originates there.

Another question to ask, is what can we do to increase visibility into the network, either by increasing end point in the machines can do package sniffing, or if we can increase doc trace, and have additional endpoint that feed to the doctor supplies, so the the doctor supplies can see what is going on there. We really need to improve our incident response. The good news is that the team is able to scale more, we have more resources, we can actually allocate more resources to this, which is what is going on. But, our ability to really deal with the situation containment is still very limited. It also highlights a lot of gaps in the current network infrastructure visualization problem, because, simple question, or the questions that we ask them don't actually have quick answers, and it also show the reality that they fly blindly in a lot of the times, and that they are also very reactive, which is the reason why we need these tools in place, because they really would allow a much more proactive, a much more understanding of what is actually, what is going on.

In a way, what is happening now, is above steam, even those the updates are made, and the AWS ability, and the dashboards, the issues are much better resolved, are much better addressed, before they even take to account.

Now, since we have a big speed state, and factory state in our environment, we really need to actually get a grip on this, because we going to have to address this problem across the board, and the fact that a lot of our networks are still flat is a big problem. We really need to contain them, we really need to get a grip with what is happening with them. Especially, because some of these services are actually third party services, which are basically controlled by suppliers, which we really need to start to get a grip on them, and start to get a much better understanding on how can we protect, and how can we reduce the attack surface of those machines. Because after all, they are the ones who are making the money, because they are the ones that are actually producing the goods for our clients.

